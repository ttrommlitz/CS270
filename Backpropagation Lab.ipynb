{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Backpropagation Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 1 Avoiding Overfit: Early Stopping and Loss Regularization\n",
        "\n",
        "### 1.1 (10%) No overfit avoidance\n",
        "Train the sklearn [MLP classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) on the [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff).  Use 3 output nodes (1 per class). Expanding the one output variable into 3 is called one-hot encoding or dummy variable encoding. There are lots of ways to implement this including the Pandas get_dummies method. This experiment is set up to run a little longer to better see the effects of overfit.  Be patient as there are lots of hidden nodes and a high max iterations setting.\n",
        "\n",
        "Use default parameters except the following:\n",
        "- hidden_layer_sizes = [64] - One hidden layer with 64 hidden nodes\n",
        "- activation = 'logistic'\n",
        "- solver = 'sgd'\n",
        "- alpha = 0\n",
        "- batch_size = 1\n",
        "- learning_rate_init = 0.01\n",
        "- shuffle = True\n",
        "- momentum = 0\n",
        "- n_iter_no_change = 50\n",
        "- max_iterations = 1000\n",
        "\n",
        "Use a random 80/20 split of the data.  Run it a few times with different random training/test splits and give average values for\n",
        "- Number of iterations until convergence\n",
        "- Training set accuracy\n",
        "- Test set accuracy\n",
        "For one run observe the softmax probabilities on the test set using clf.predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     b'Iris-setosa'  b'Iris-versicolor'  b'Iris-virginica'\n",
            "0              True               False              False\n",
            "1              True               False              False\n",
            "2              True               False              False\n",
            "3              True               False              False\n",
            "4              True               False              False\n",
            "..              ...                 ...                ...\n",
            "145           False               False               True\n",
            "146           False               False               True\n",
            "147           False               False               True\n",
            "148           False               False               True\n",
            "149           False               False               True\n",
            "\n",
            "[150 rows x 3 columns]\n",
            "Average number of iterations to converge: 335.0\n",
            "Average training set accuracy: 0.9683333333333334\n",
            "Average test set accuracy: 0.9666666666666666\n",
            "Softmax probabilities for the first run:\n",
            " [[4.25540460e-06 4.90554198e-02 9.74698948e-01]\n",
            " [4.34975093e-03 9.97510684e-01 4.39928824e-05]\n",
            " [9.99533742e-01 2.58095370e-03 9.80211400e-12]\n",
            " [2.49448993e-05 4.85862706e-01 4.82529886e-01]\n",
            " [1.00901713e-05 1.70159186e-01 8.63211960e-01]\n",
            " [2.50224255e-04 9.82254106e-01 6.92368968e-03]\n",
            " [5.57845880e-06 8.62443189e-02 9.54678895e-01]\n",
            " [9.99107785e-01 9.38710332e-03 1.97294413e-11]\n",
            " [8.49665442e-04 9.93626204e-01 8.82081315e-04]\n",
            " [9.84112911e-04 9.96596979e-01 4.78181205e-04]\n",
            " [3.31071288e-04 9.81614693e-01 4.67493177e-03]\n",
            " [1.82149650e-04 9.61421627e-01 1.44585131e-02]\n",
            " [9.98822527e-01 9.46715387e-03 2.99550046e-11]\n",
            " [1.06253572e-03 9.95745313e-01 4.52708808e-04]\n",
            " [4.71838715e-04 9.92925344e-01 1.83116550e-03]\n",
            " [9.99502017e-01 3.10182696e-03 1.02118259e-11]\n",
            " [9.09640462e-06 2.03343938e-01 8.89787200e-01]\n",
            " [2.03944496e-05 4.10061381e-01 5.75608955e-01]\n",
            " [1.11326319e-04 9.02202103e-01 4.16402795e-02]\n",
            " [8.17976043e-06 1.56490852e-01 9.01121855e-01]\n",
            " [4.74307516e-06 6.17767731e-02 9.65283378e-01]\n",
            " [9.98718106e-01 1.03088624e-02 3.21385673e-11]\n",
            " [9.99407557e-01 2.96287394e-03 1.36074688e-11]\n",
            " [9.68083326e-06 1.86041629e-01 8.62462279e-01]\n",
            " [9.99107785e-01 9.38710332e-03 1.97294413e-11]\n",
            " [1.55606279e-04 9.60188055e-01 1.88460443e-02]\n",
            " [2.76388669e-06 3.17388695e-02 9.87125323e-01]\n",
            " [9.99748565e-01 1.21694987e-03 4.38510330e-12]\n",
            " [2.27214355e-04 9.69303481e-01 1.05935067e-02]\n",
            " [4.36702710e-06 4.45436421e-02 9.72041750e-01]]\n"
          ]
        }
      ],
      "source": [
        "#Iris with no regularization\n",
        "PATH_TO_IRIS_ARFF = 'datasets/iris.arff'\n",
        "\n",
        "iris_arff = arff.loadarff(PATH_TO_IRIS_ARFF)\n",
        "iris_df = pd.DataFrame(iris_arff[0])\n",
        "\n",
        "X = iris_df.iloc[:, :-1].values\n",
        "y = pd.get_dummies(iris_df.iloc[:, -1])\n",
        "\n",
        "print(y)\n",
        "\n",
        "y = y.to_numpy()\n",
        "\n",
        "num_iter_to_converge = []\n",
        "training_set_accuracy = []\n",
        "test_set_accuracy = []\n",
        "\n",
        "softmax_probability = None\n",
        "\n",
        "for i in range(5):\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[64], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=.01, shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000\n",
        "  )\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf.fit(X_train, y_train)\n",
        "  num_iter_to_converge.append(clf.n_iter_)\n",
        "  training_set_accuracy.append(clf.score(X_train, y_train))\n",
        "  test_set_accuracy.append(clf.score(X_test, y_test))\n",
        "\n",
        "  \n",
        "  if i == 0:\n",
        "    softmax_probability = clf.predict_proba(X_test)\n",
        "\n",
        "print(f'Average number of iterations to converge: {np.mean(num_iter_to_converge)}')\n",
        "print(f'Average training set accuracy: {np.mean(training_set_accuracy)}')\n",
        "print(f'Average test set accuracy: {np.mean(test_set_accuracy)}')\n",
        "\n",
        "print('Softmax probabilities for the first run:\\n', softmax_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion*\n",
        "\n",
        "The model takes on average 309 iterations to converge. This means that on average, the classifier runs for 309 epochs before not improving. Note that we are using the n_iter_no_change hyperparameter and setting it to 50, meaning that the classifier will run for 50 iterations without improvement, so in reality, the model stopped improving at 259 iterations. \n",
        "\n",
        "The average training set accuracy is .983. This means that the model correctly classifies 98.3% of the training set after it has been trained. This is a good rate of accuracy, but it is important to note that the model is being trained on the training set, so it is expected to have a high accuracy.\n",
        "\n",
        "The average test set accuracy is .953. This means that the model correctly classifies 95.3% of the test set. This is a good rate of accuracy. The test set consists of data that the model has not seen before, so we expect the accuracy to be lower than the training set accuracy. However, the rate is still high, so the model is generalizing well.\n",
        "\n",
        "The softmax probabilities shown above are the probabilities that the given instance is a member of each output class. Each row in the softmax 2d array has 3 elements corresponding to the probabilities of the instance being a member of each class. The highest probability is the class that the model predicts the instance to be a member of. For example, the first row in the softmax array has a probability of .00024 for the first class, .993 for the second class, and .0019 for the third class. This means that the model predicts the instance to be a member of the second class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 (10%) Early Stopping (Validation Set)\n",
        "\n",
        "- Do the same as above but his time with early stopping\n",
        "- Use a validation set taken from the training set for your stopping criteria. Using 10-15% of the training set for a validation set is common. You do this simply by setting the MLPClassifier early_stopping, validation_fraction, and n_iter_no_change parameters.\n",
        "- Run it a few times with different training/test splits and give average values for\n",
        "    - Number of iterations until convergence\n",
        "    - Training set accuracy\n",
        "    - Test set accuracy\n",
        "    - Best validation score (MLPClassifer attribute best_validation_score_)\n",
        "- For one run create a graph with validation set accuracy (*y*-axis) vs epochs (*x*-axis). Hint: MLPClassifer attribute validation_scores_\n",
        "\n",
        "Note: Due to the simplicity of and lack of noise in the iris data set you will not see the accuracy improvements that early stopping or loss regularization can give for more complex noisy datasets.  In particular, early stopping will have lower than expected results because with a very small VS taken from a very small training set there is less data to train on and more variance with the VS score.  Thus, you will probably get lower accuracies for VS than normal training for this less typical case.  But at least you will get practice on using early stopping and loss regularization for future data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average number of iterations to converge: 112.0\n",
            "Average training set accuracy: 0.925\n",
            "Average test set accuracy: 0.9133333333333333\n",
            "Average best validation score: 0.9466666666666667\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x15de60d10>]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Iris with early stopping and validation scores graph\n",
        "PATH_TO_IRIS_ARFF = 'datasets/iris.arff'\n",
        "\n",
        "iris_arff = arff.loadarff(PATH_TO_IRIS_ARFF)\n",
        "iris_df = pd.DataFrame(iris_arff[0])\n",
        "\n",
        "X = iris_df.iloc[:, :-1].values\n",
        "y = pd.get_dummies(iris_df.iloc[:, -1].values).to_numpy()\n",
        "\n",
        "num_iter_to_converge = []\n",
        "training_set_accuracy = []\n",
        "test_set_accuracy = []\n",
        "best_validation_scores = []\n",
        "\n",
        "run_1_validation_scores = None\n",
        "run_1_epochs = None\n",
        "\n",
        "for i in range(5):\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[64], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=.01, shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000,\n",
        "    early_stopping=True, validation_fraction=.125\n",
        "  )\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf.fit(X_train, y_train)\n",
        "  num_iter_to_converge.append(clf.n_iter_)\n",
        "  training_set_accuracy.append(clf.score(X_train, y_train))\n",
        "  test_set_accuracy.append(clf.score(X_test, y_test))\n",
        "  best_validation_scores.append(clf.best_validation_score_)\n",
        "\n",
        "  if i == 0:\n",
        "    run_1_validation_scores = clf.validation_scores_\n",
        "    run_1_epochs = clf.n_iter_\n",
        "\n",
        "print(f'Average number of iterations to converge: {np.mean(num_iter_to_converge)}')\n",
        "print(f'Average training set accuracy: {np.mean(training_set_accuracy)}')\n",
        "print(f'Average test set accuracy: {np.mean(test_set_accuracy)}')\n",
        "print(f'Average best validation score: {np.mean(best_validation_scores)}')\n",
        "\n",
        "# Validation scores graph\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Score')\n",
        "plt.title('First Run Validation Scores vs Epochs')\n",
        "plt.plot(range(run_1_epochs), run_1_validation_scores)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNMATZqjtlxs"
      },
      "source": [
        "*Discussion of average values above and the validation score graph*\n",
        "\n",
        "The average number of iterations until convergence is 112. This means that on average, the classifier runs for 112 epochs before not improving. Again, we are using the n_iter_no_change hyperparameter.\n",
        "\n",
        "The average training set accuracy is .925. This means that the model correctly classifies 92.5% of the training set after it has been trained. This is a good rate of accuracy, but similar to above, the model is being trained on the training set, so it is expected to have a high accuracy.\n",
        "\n",
        "The average test set accuracy is .913. This means that the model correctly classifies 91.3% of the test set. This is also a good rate of accuracy. \n",
        "As the problem description states, the benefits of early stopping are likely not as apparent in this dataset. The iris dataset is simple and has little noise, so the model is likely to converge quickly and not overfit. Because the iris dataset is so simple, it is surprising that the test set accuracy is not even higher. Being just above 90% does not seem to be a good result for this dataset, and it seems to be struggling to generalize. My hypothesis is that this is because there are not enough instances in the dataset to train the model well.\n",
        "\n",
        "The average best validation score is .947. On each epoch of training, the model is tested against the validation set and scored. We trained 5 different times, and each time, the model had a best validation score. These 5 best validation scores were averaged to get the average best validation score. On average, across different training and test splits, the model had a best validation score of .947, which is a good rate of accuracy. As the graph shows, the validation scores increased from 0 to .947 over the course of training for a given train test split. This is expected, as the model is being trained and therefore should be improving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 (10%) Loss Regularization\n",
        "\n",
        "- Do the same as in 1.1 but his time with loss regularization (Do not do early stopping)\n",
        "- Run it with different L2 regularization parameter values (alpha).  The default for alpha is .0001.  Try other values such as .1, .01, .001, .00001, etc. Make a table with each row including:\n",
        "    - The regularization parameter value\n",
        "    - Number of iterations until convergence\n",
        "    - Training set accuracy\n",
        "    - Test set accuracy\n",
        "    - Best loss value (MLPClassifer attribute best_loss_)\n",
        "- Which regularization value gave you the best results?\n",
        "- For your best regularization value do one run and create a graph with loss (*y*-axis) vs epochs (*x*-axis) for the training set (Hint: MLPClassifer attribute loss_curve_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------+------------------------------------+-------------------------+---------------------+-------------------+\n",
            "|   Regularization Value |   Number of Iterations to Converge |   Training Set Accuracy |   Test Set Accuracy |   Best Loss Value |\n",
            "+========================+====================================+=========================+=====================+===================+\n",
            "|                 0.1    |                                164 |                0.658333 |            0.566667 |          1.62431  |\n",
            "+------------------------+------------------------------------+-------------------------+---------------------+-------------------+\n",
            "|                 0.01   |                                225 |                0.958333 |            0.966667 |          0.797233 |\n",
            "+------------------------+------------------------------------+-------------------------+---------------------+-------------------+\n",
            "|                 0.001  |                                258 |                0.966667 |            1        |          0.256601 |\n",
            "+------------------------+------------------------------------+-------------------------+---------------------+-------------------+\n",
            "|                 0.0001 |                                278 |                0.975    |            1        |          0.156333 |\n",
            "+------------------------+------------------------------------+-------------------------+---------------------+-------------------+\n",
            "|                 1e-05  |                                345 |                0.991667 |            0.966667 |          0.122141 |\n",
            "+------------------------+------------------------------------+-------------------------+---------------------+-------------------+\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x15dee3950>]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Iris with Loss Regularization \n",
        "PATH_TO_IRIS_ARFF = 'datasets/iris.arff'\n",
        "\n",
        "iris_arff = arff.loadarff(PATH_TO_IRIS_ARFF)\n",
        "iris_df = pd.DataFrame(iris_arff[0])\n",
        "\n",
        "X = iris_df.iloc[:, :-1].values\n",
        "y = pd.get_dummies(iris_df.iloc[:, -1].values).to_numpy()\n",
        "\n",
        "regularization_values = [.1, .01, .001, .0001, .00001]\n",
        "\n",
        "table_head = ['Regularization Value', 'Number of Iterations to Converge', 'Training Set Accuracy', 'Test Set Accuracy', 'Best Loss Value']\n",
        "table_results = []\n",
        "\n",
        "best_loss_curve = None\n",
        "best_test_accuracy = 0\n",
        "\n",
        "for i in range(5):\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[64], activation='logistic', solver='sgd',\n",
        "    alpha=regularization_values[i], batch_size=1, learning_rate_init=.01, shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000\n",
        "  )\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf.fit(X_train, y_train)\n",
        "  table_results.append([regularization_values[i], clf.n_iter_, clf.score(X_train, y_train), clf.score(X_test, y_test), clf.best_loss_])\n",
        "  if clf.score(X_test, y_test) > best_test_accuracy:\n",
        "    best_test_accuracy = clf.score(X_test, y_test)\n",
        "    best_loss_curve = clf.loss_curve_\n",
        "\n",
        "\n",
        "print(tabulate(table_results, headers=table_head, tablefmt='grid'))\n",
        "\n",
        "# plot best loss curve\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Best Loss Curve')\n",
        "plt.plot(range(len(best_loss_curve)), best_loss_curve)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion and comparison between no regularization, early stopping and loss regularization*\n",
        "\n",
        "The regularization value that gave the best results was .00001. This regularization value gave the lowest loss value. It is interesting to me that this regularization value did not actually give the best test set accuracy, which was 1 (given by multiple other regularization values). I believe this is simply due to random sampling of the train and test sets, as .97 is still quite close to 1. \n",
        "\n",
        "1.1 deals with no regularization. 1.2 deals with early stopping. 1.3 deals with loss regularization. No regularization means that the model is not penalized for having large weights, and it can lead to overfitting. Early stopping means that the model uses a validation set when training and stops training when the validation set accuracy stops improving. This can help avoid overfitting because the weights have less time to grow. Loss regularization means that the model drives weights to be small (sometimes 0), which helps avoid overfitting. This also helps in feature reduction, because features that are not important will have weights driven to 0 and can then be removed from the dataset or model.\n",
        "\n",
        "1.1 had a test set accuracy of .953. 1.2 had a test set accuracy of .913. 1.3 had a test set accuracy of 1. This means that the model with loss regularization had the best test accuracy. It is surprising to me that the model with no regularization and no early stopping (1.1) outperformed the model with early stopping (1.2). I suspect that this is due to the simplicity of the iris dataset, and the fact that the model is not likely to overfit either way, and this is just a random result for a certain train test split. The model with loss regularization performed the best, which makes sense. It is my hypothesis that the model pushed some weights to be 0, which helped the model generalize better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOteTlV6S0bq"
      },
      "source": [
        "## 2 Hyperparameters \n",
        "In this section we use the [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff) to consider the hyperparameters of learning rate, number of hidden nodes, and momentum. \n",
        "\n",
        "### 2.1 (10%) Vowel Dataset Questions\n",
        "- Give the baseline accuracies for the Iris and Vowel datasets. Baseline accuracy is what you would get if the model just outputs the majority class of the data set (i.e. the output value which occurs most often). These two data sets are not great example for this as they have an equal amount of each class, which is not typical.\n",
        "- Discuss why the vowel data set will probably have lower accuracy than Iris.\n",
        "- Consider which of the vowel dataset's input features you should not use in training and discuss why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris Baseline Accuracy: 0.3333333333333333\n",
            "Vowel Baseline Accuracy: 0.09090909090909091\n"
          ]
        }
      ],
      "source": [
        "PATH_TO_IRIS_ARFF = 'datasets/iris.arff'\n",
        "iris_arff = arff.loadarff(PATH_TO_IRIS_ARFF)\n",
        "iris_df = pd.DataFrame(iris_arff[0])\n",
        "\n",
        "PATH_TO_VOWEL_ARFF = 'datasets/vowel.arff'\n",
        "vowel_arff = arff.loadarff(PATH_TO_VOWEL_ARFF)\n",
        "vowel_df = pd.DataFrame(vowel_arff[0])\n",
        "\n",
        "iris_baseline_accuracy = iris_df['class'].value_counts().max() / iris_df.shape[0]\n",
        "vowel_baseline_accuracy = vowel_df['Class'].value_counts().max() / vowel_df.shape[0]\n",
        "\n",
        "print(f'Iris Baseline Accuracy: {iris_baseline_accuracy}')\n",
        "print(f'Vowel Baseline Accuracy: {vowel_baseline_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmq9GSbJS8k2"
      },
      "source": [
        "\n",
        "*Discuss these items here*\n",
        "\n",
        "The iris baseline accuracy is .333, which is expected because there are 3 evenly distributed output classes. The vowel baseline accuracy is .0909, which is expected because there are 11 evenly distributed output classes (1/11 = .0909). The vowel dataset has 11 classes while the iris dataset has 3. The vowel dataset will likely have a lower accuracy because there are more classes to choose from, and as such, the softmax probabilities will be lower and more spread out. Also, because the vowel dataset has a lower baseline accuracy, when the model is forced to make a guess, it will be less likely to guess correctly. In addition, it has some noisy features that are not relevant to the classification task. \n",
        "\n",
        "Features in the vowel dataset that are not relevant to the classification task are the first 3 features, which are 'Train or Test', 'Speaker Number', and 'Sex'. These features are simply metadata that mark identifying information about the instance and are not helpful in classifying instances. They are noisy features that only make classification harder. As such, they should not be used in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "### 2.2 (10%) Learning Rate\n",
        "Load the [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff). Drop any features which you explained above as being inappropriate for training.\n",
        "\n",
        "Hints: Consider the Pandas drop method for dropping columns. When you want to transform features in your data set there are lots of approaches. You could edit the arff file directly, or make the transforms in your code.  The Pandas replace method is nice for that. For example, if you wanted to change the vowel data set gender feature in a Pandas dataframe to 0/1 you could do the following:\n",
        "\n",
        "vowel_df['Sex'] = vowel_df['Sex'].str.decode('utf-8')   //Changes the byte code data into a normal string, b'Male' becomes \"Male\"\\\n",
        "vowel_df = vowel_df.replace('Male', 0)\\\n",
        "vowel_df = vowel_df.replace('Female', 1)\n",
        "\n",
        "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
        "- Use a random 75/25 split of the data for the training/test set.\n",
        "- Do not use early stopping.\n",
        "- Try at least 5 different learning rates (LR) from very small (e.g. .001) to pretty big (e.g. 10). Each LR will require a different number of epochs to learn. LR effects both accuracy and time required for learning.\n",
        "- Create a table which includes a row for each LR.  Your table columns should be LR, # epochs to learn the model, final training set accuracy and final test set accuracy.  As learning rates get smaller, it usually takes more epochs to learn. If your model is stopping learning too soon (converging) by hitting max_iterations (in this case and in experiments below), then you need to increase your max_iterations parameter in order to give your model a fair chance.  To keep things faster, you don't need to increase max_iter past 1000 if you don't want to, but point out when more iterations may have given improvement.\n",
        "\n",
        "In real testing one averages the results of multiple trials per LR (and other parameters) with different intitial conditions (training/test split, initial weights, etc.). That gives more accurate results but is not required for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KBGUn43ASiXW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------+-------------------------+---------------------+\n",
            "|   Learning Rate |   # Epochs |   Training Set Accuracy |   Test Set Accuracy |\n",
            "+=================+============+=========================+=====================+\n",
            "|           0.001 |       1000 |                0.474394 |            0.443548 |\n",
            "+-----------------+------------+-------------------------+---------------------+\n",
            "|           0.01  |       1000 |                0.838275 |            0.75     |\n",
            "+-----------------+------------+-------------------------+---------------------+\n",
            "|           0.1   |        357 |                0.839623 |            0.725806 |\n",
            "+-----------------+------------+-------------------------+---------------------+\n",
            "|           1     |        114 |                0.412399 |            0.391129 |\n",
            "+-----------------+------------+-------------------------+---------------------+\n",
            "|          10     |         75 |                0.083558 |            0.112903 |\n",
            "+-----------------+------------+-------------------------+---------------------+\n",
            "Best learning rate: 0.01\n"
          ]
        }
      ],
      "source": [
        "PATH_TO_VOWEL_ARFF = 'datasets/vowel.arff'\n",
        "vowel_arff = arff.loadarff(PATH_TO_VOWEL_ARFF)\n",
        "vowel_df = pd.DataFrame(vowel_arff[0])\n",
        "vowel_df = vowel_df.drop(columns=['Train or Test', 'Speaker Number', 'Sex'], axis=1)\n",
        "\n",
        "table_head = ['Learning Rate', '# Epochs', 'Training Set Accuracy', 'Test Set Accuracy']\n",
        "table_results = []\n",
        "\n",
        "learning_rates = [.001, .01, .1, 1, 10]\n",
        "best_test_accuracy = 0\n",
        "best_lr = 0\n",
        "\n",
        "for i in range(5):\n",
        "  NUM_FEATURES = 7\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[NUM_FEATURES * 2], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=learning_rates[i], shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000\n",
        "  )\n",
        "\n",
        "  X = vowel_df.iloc[:, :-1].values\n",
        "  y = pd.get_dummies(vowel_df.iloc[:, -1].values).to_numpy()\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "  clf.fit(X_train, y_train)\n",
        "  table_results.append([learning_rates[i], clf.n_iter_, clf.score(X_train, y_train), clf.score(X_test, y_test)])\n",
        "  if clf.score(X_test, y_test) > best_test_accuracy:\n",
        "    best_test_accuracy = clf.score(X_test, y_test)\n",
        "    best_lr = learning_rates[i]\n",
        "\n",
        "print(tabulate(table_results, headers=table_head, tablefmt='grid'))\n",
        "print(f'Best learning rate: {best_lr}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpjJBIwktlxt"
      },
      "source": [
        "*Discuss your table and the effect of different learning rates on both training time and accuracy*\n",
        "\n",
        "There is a sweet spot for learning rates. The learning rates the performed the worst were .001, 1, and 10. The learning rate the performed the best was .01. The pattern is that as learning rates increase, the accuracy increases up to a certain level, then begins to decrease again. This makes sense, because when the learning rate is too small, the model hits the max number of iterations because it is not learning fast enough. When the learning rate is too large, the model learns too quickly and cannot find the optimal set of weights. For the smallest learning rates (.001 and .01), more iterations would have given improvement in the accuracy scores because the model was not able to converge in the max number of iterations. The learning rate that performed the best across training/test accuracy was .1. At this rate, the model converged but still had a good accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-nUu5Txtlxt"
      },
      "source": [
        "### 2.3 (10%) Number of Hidden Nodes\n",
        "\n",
        "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
        "\n",
        "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy. \n",
        "- Create a table just like above, except with # of hidden nodes rather than LR.\n",
        "\n",
        "In general, whenever you are testing a parameter such as # of hidden nodes, keep testing values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer got improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uLqeA1iutlxt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------+-------------------------+---------------------+\n",
            "|   # Hidden Nodes |   # Epochs |   Training Set Accuracy |   Test Set Accuracy |\n",
            "+==================+============+=========================+=====================+\n",
            "|                1 |        425 |                0        |            0        |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|                2 |        675 |                0.142857 |            0.104839 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|                4 |       1000 |                0.260108 |            0.193548 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|                8 |       1000 |                0.660377 |            0.560484 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|               16 |       1000 |                0.873315 |            0.733871 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|               32 |       1000 |                0.986523 |            0.822581 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|               64 |       1000 |                0.998652 |            0.862903 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|              128 |       1000 |                1        |            0.866935 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|              256 |       1000 |                0.998652 |            0.919355 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|              512 |       1000 |                1        |            0.870968 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "best number of hidden nodes: 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Train with different numbers of hidden nodes\n",
        "table_head = ['# Hidden Nodes', '# Epochs', 'Training Set Accuracy', 'Test Set Accuracy']\n",
        "table_results = []\n",
        "\n",
        "continue_training = True\n",
        "num_hidden_nodes = 1\n",
        "current_accuracy = 0\n",
        "\n",
        "while continue_training:\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[num_hidden_nodes], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=best_lr, shuffle=True,\n",
        "    max_iter=1000, momentum=0, n_iter_no_change=50\n",
        "  )\n",
        "\n",
        "  X = vowel_df.iloc[:, :-1].values\n",
        "  y = pd.get_dummies(vowel_df.iloc[:, -1].values).to_numpy()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "  clf.fit(X_train, y_train)\n",
        "  table_results.append([num_hidden_nodes, clf.n_iter_, clf.score(X_train, y_train), clf.score(X_test, y_test)])\n",
        "\n",
        "  # continue training until there is no more improvement in accuracy\n",
        "  if clf.score(X_test, y_test) <= current_accuracy and num_hidden_nodes > 4:\n",
        "    continue_training = False\n",
        "  else:\n",
        "    current_accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "  num_hidden_nodes *= 2\n",
        "\n",
        "\n",
        "print(tabulate(table_results, headers=table_head, tablefmt='grid'))\n",
        "\n",
        "# best number of hidden nodes is the first item in the second to last row of the table\n",
        "best_num_hidden_nodes = table_results[-2][0]\n",
        "print(f'best number of hidden nodes: {best_num_hidden_nodes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLliSMtXtlxt"
      },
      "source": [
        "*Discuss your table and the effect of different numbers of hidden nodes on both training time and accuracy*\n",
        "\n",
        "The best number of hidden nodes that I discovered was 256. As the number of hidden nodes in the single hidden layer increases, the accuracy increases up to a certain point, then begins to decrease again, similar to the learning rate. This is because as the number of hidden nodes increases, the model is able to learn more complex patterns in the data, but after a certain point, the model is overfitting. The model is learning the training set too well and is not generalizing well to the test set. We can see this because when using 512 hidden nodes, we get a perfect training set accuracy but the test set accuracy decreases, which is a sign of overfitting. Also when using very few hidden nodes, such as 2, the model performs very poorly because there is limited capacity to represent and learn patterns in the data.\n",
        "\n",
        "For the later experiments with more hidden nodes, the model was not able to converge in the max number of iterations. This is because it was using a smaller learning rate of .01. Its possible that if more iterations were allowed, the model would have converged and performed better with more hidden nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v72ryeHXtlxu"
      },
      "source": [
        "### 2.4 (10%) Momentum\n",
        "\n",
        "Try at least 5 different momentum terms between 0 and just less than 1 using the best number of hidden nodes and LR from your earlier experiments.\n",
        "\n",
        "- Create a table just like above, except with momentum values rather than LR or number of hidden nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yiEBTL6Vtlxu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------+-------------------------+---------------------+\n",
            "|   Momentum Value |   # Epochs |   Training Set Accuracy |   Test Set Accuracy |\n",
            "+==================+============+=========================+=====================+\n",
            "|             0    |       1000 |                0.998652 |            0.907258 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|             0.2  |       1000 |                1        |            0.895161 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|             0.4  |       1000 |                1        |            0.883065 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|             0.6  |       1000 |                1        |            0.943548 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|             0.8  |        809 |                1        |            0.947581 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "|             0.99 |         54 |                0.107817 |            0.100806 |\n",
            "+------------------+------------+-------------------------+---------------------+\n",
            "Best momentum value: 0.8\n"
          ]
        }
      ],
      "source": [
        "# Train with different momentum values\n",
        "\n",
        "table_head = ['Momentum Value', '# Epochs', 'Training Set Accuracy', 'Test Set Accuracy']\n",
        "table_results = []\n",
        "\n",
        "momentums = [0, .2, .4, .6, .8, .99]\n",
        "best_test_accuracy = 0\n",
        "best_momentum_value = 0\n",
        "\n",
        "for i in range(6):\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[best_num_hidden_nodes], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=best_lr, shuffle=True,\n",
        "    momentum=momentums[i], n_iter_no_change=50, max_iter=1000\n",
        "  )\n",
        "\n",
        "  X = vowel_df.iloc[:, :-1].values\n",
        "  y = pd.get_dummies(vowel_df.iloc[:, -1].values).to_numpy()\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "  clf.fit(X_train, y_train)\n",
        "  table_results.append([momentums[i], clf.n_iter_, clf.score(X_train, y_train), clf.score(X_test, y_test)])\n",
        "  if clf.score(X_test, y_test) > best_test_accuracy:\n",
        "    best_test_accuracy = clf.score(X_test, y_test)\n",
        "    best_momentum_value = momentums[i]\n",
        "\n",
        "print(tabulate(table_results, headers=table_head, tablefmt='grid'))\n",
        "print(f'Best momentum value: {best_momentum_value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqidhekCtlxu"
      },
      "source": [
        "*Discuss your table and the effect of momentum on both training time and accuracy*\n",
        "\n",
        "The best momentum value that I discovered was .8, which had a test set accuracy of .948. The worst one that I found was .99. As momentum increases from 0, the accuracy stays relatively stable, hovering between .88 and .94. However, after a momentum value of .8, the accuracy drops off a cliff. When momentum reaches .99, the test set accuracy is .1.\n",
        "\n",
        "Momentum is a hyperparameter used to speed up convergence of the loss function when using stochastic gradient descent. I suspect that when momentum is too high, the model cannot find the minimum of the loss function and is not able to converge because it is jumping around too much. This is why the accuracy drops off so much when the momentum is .99.\n",
        "\n",
        "Most of the momentum values did not converge in the max number of iterations. Again, this is likely due to the learning rate being .01. If more iterations were allowed, the model may have converged and performed better with the earlier momentum values. However, even still, these values performed well with high accuracy values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hHxNgUCtlxv"
      },
      "source": [
        "### 2.5 (10%) Automatic Hyperparameter Discovery\n",
        "Using the vowel dataset, automatically adjust the LR, # of hidden nodes, and momentum using [grid and random search](https://scikit-learn.org/stable/modules/grid_search.html) \n",
        "- For grid search include the most promising hyperparameter values you used in your experiments above.  You may add others also.\n",
        "- Be patient as the grid search can take a while since it has to train all combinations of models. Don't use too many parameter options or it will too slow.\n",
        "- Report your best hyperparameters and accuracy.  Unfortunately, you will not always get as high a score as you might expect.  This is in part due to the simplicity of the dataset.  It also teaches that in gerneral you should not always blindly assume that a tool will get you the results you expect and that you may need to consider multiple approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hidden_layer_sizes': [32], 'learning_rate_init': 0.1, 'momentum': 0.4}\n",
            "0.40909090909090906\n"
          ]
        }
      ],
      "source": [
        "#Grid search for hyperparameters.\n",
        "#Here is one variation of code you could use for your grid search. You can try your own variation if you prefer.\n",
        "\n",
        "PATH_TO_VOWEL_ARFF = 'datasets/vowel.arff'\n",
        "vowel_arff = arff.loadarff(PATH_TO_VOWEL_ARFF)\n",
        "vowel_df = pd.DataFrame(vowel_arff[0])\n",
        "vowel_df = vowel_df.drop(columns=['Train or Test', 'Speaker Number', 'Sex'], axis=1)\n",
        "\n",
        "X = vowel_df.iloc[:, :-1].values\n",
        "Y = pd.get_dummies(vowel_df.iloc[:, -1].values).to_numpy()\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = MLPClassifier(activation='logistic', solver='sgd',alpha=0,early_stopping=True, n_iter_no_change=10, batch_size=1)\n",
        "parameters = {'learning_rate_init':( .001, best_lr, .1), #You have to fill in the rest of your values for these lists\n",
        "              'hidden_layer_sizes': ([8], [32], [best_num_hidden_nodes]),\n",
        "              'momentum':(0, .4, best_momentum_value)}\n",
        "grid = GridSearchCV(clf, parameters)\n",
        "grid.fit(X,Y)    #This takes a while to run\n",
        "print(grid.best_params_)\n",
        "print(grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hidden_layer_sizes': [8], 'learning_rate_init': 0.12217339652586207, 'momentum': 0.5145446073604273}\n",
            "0.39494949494949494\n"
          ]
        }
      ],
      "source": [
        "#Randomized search for hyperparameters\n",
        "#Here is one variation of code you could use for your randomized search.\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "clf = MLPClassifier(activation='logistic', solver='sgd',alpha=0,early_stopping=True, n_iter_no_change=10, batch_size=1)\n",
        "distributions = dict(learning_rate_init=uniform(loc=.001, scale=.999), #loc is the min val, and loc + scale is the max val\n",
        "                    hidden_layer_sizes = ([8], [16], [32]), #since there is no distribution it samples these values uniformly\n",
        "                    momentum=uniform(loc=0,scale =.99))\n",
        "search = RandomizedSearchCV(clf, distributions, n_iter=10)\n",
        "search.fit(X,Y)\n",
        "print(search.best_params_)\n",
        "print(search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Discussion and comparison of grid and randomized parameter search*\n",
        "\n",
        "Grid search and randomized parameter search are two approaches to tuning a model (finding the best hyperparameters). When performing a grid search, we specify possible values for hyperparameters and the model trains against all possible combinations of those models to find the best one. When performing a randomized parameter search, we specify a range for continuous hyperparameters and the model randomly samples from that range when testing random combinations of hyperparameters.\n",
        "\n",
        "The grid search approach found that the best hyperparameters were 32 nodes in the hidden layer, .1 for the learning rate, and .4 for the momentum, with a best score of .41. The randomized parameter search approach found that the best hyperparameters were 32 nodes in the hidden layer, .122 for the learning rate, and .515 for the momentum, with a best score of .39.\n",
        "\n",
        "I am surprised that both search methods performed badly. I suspect that this is because the search space is too small. If we included more hyperparameters and more possible values for those hyperparameters, we would likely achieve better results at the cost of longer training times. \n",
        "\n",
        "Both methods found similar learning rates and momentum values, but the grid search found a better score when using 32 nodes. I am surprised that the randomized search did not find a better when using 32 hidden nodes as well, because that was an option for the search space. My guess is that the slightly higher momentum value that the randomized search found meant that 32 hidden nodes performed worse than 8, although I'm not sure of the full reason why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIM0rEA9tlxu"
      },
      "source": [
        "## 3 Regression with MLPs\n",
        "\n",
        "### 3.1 (10%) - Learn a regression data set of your choice\n",
        "\n",
        "Train MLP on any real world data set that requires regression (i.e. has a real valued ouput) and discuss your effort and results.  While the [Irvine ML Repository](https://archive.ics.uci.edu) is a great resource, also onsider [Kaggle](https://www.kaggle.com) and [OpenML](https://openml.org) as other great place to find datasets.\n",
        "- Use [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) rather than MLPclassifier.  It has almost the exact same setup as MLPClassier except it uses the linear activation function for the output nodes and SSE as the loss function.  MLPClassier uses softmax activation for the output nodes and cross-entropy for the loss function.\n",
        "- Use any reasonable hyperparameters that you want.  \n",
        "- You will probably need to normalize input features.\n",
        "- It is not typically necessary to normalize the output.\n",
        "- Split into train and test and report the training and test set MAEs (Mean Absolute Error). For regression problems where we don't normalize the output, MAE is an intuitive measure as it shows exactly how much our output is off on average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Set MAE: 5.095052323297902\n",
            "Test Set MAE: 5.49513481326512\n"
          ]
        }
      ],
      "source": [
        "# Load and Learn a real world regression data set\n",
        "# To calculate MAE you could do a variation of the following\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "concrete_compressive_strength = fetch_ucirepo(id=165)\n",
        "\n",
        "clf = MLPRegressor(\n",
        "    hidden_layer_sizes=[64], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=.01, shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000\n",
        ")\n",
        "\n",
        "X = concrete_compressive_strength.data.features\n",
        "y = concrete_compressive_strength.data.targets.to_numpy()\n",
        "\n",
        "X = normalize(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "clf.fit(X_train, y_train)\n",
        "print('Training Set MAE:', mean_absolute_error(clf.predict(X_train), y_train))\n",
        "print('Test Set MAE:', mean_absolute_error(clf.predict(X_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion*\n",
        "\n",
        "Mean Absolute Error (MAE) is a measure of how far off the model's predictions are from the actual values. For each instance, the absolute error (|actual - predicted|) is calculated, and then the average of these errors is taken across all instances. Lower values of MAE indicate that the model's predictions are closer to the actual values.\n",
        "\n",
        "For this model, the training set MAE was 5.1 and the test set MAE was 5.5. This means that on average, the model's predictions were about 5.1 units off from the actual values in the training set, and about 5.5 units off from the actual values in the test set.\n",
        "\n",
        "Before normalizing the input features, the MAE was about 14 for the training set and 13 for the test set. This means that normalization helped the model make better predictions, because the MAEs dropped to 5.1 and 5.5 respectively. The targets ranged from 2.33 to 82.6, with a mean and median of about 35. The standard deviation of the targets is 16.7, so the data is pretty well spread out. This means that our predictions are relatively close to the actual values, and the model is performing well, however, there is room for improvement. One strategy could be to use standardization instead of normalization of the input features to see if that helps reduce the MAE further. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 (10%) - Other MLP Hyperparameters \n",
        "With the same data set, you may (not required) experiment with some of the hyperparameters you already did above (LR, hidden nodes, momentum, validation set parameters, regularization).  But for sure experiment with and discuss the results of the first two hyperparameters below (activation functions and multiple hidden layers).  We encourage you to experiment briefly with the others but they are not required. \n",
        "\n",
        "- different hidden layer activation functions (tanh, relu in addition to logistic) - Note that Sklean does not currently let you choose the output layer activation function.  It is automatically softmax for classification and linear for regression.\n",
        "- more than one hidden layer\n",
        "- solver - try adam and lbfgs in addition to sgd\n",
        "- batch size\n",
        "- learning rate adaptation - this is the schedule parameter which lets LR adapt during learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hu1JE4vStlxv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+------------+--------------------+----------------+\n",
            "| Activation Function   |   # Epochs |   Training Set MAE |   Test Set MAE |\n",
            "+=======================+============+====================+================+\n",
            "| logistic              |        633 |            5.27141 |        5.18165 |\n",
            "+-----------------------+------------+--------------------+----------------+\n",
            "| tanh                  |        452 |            5.5409  |        5.71031 |\n",
            "+-----------------------+------------+--------------------+----------------+\n",
            "| relu                  |        200 |            9.66832 |        9.48162 |\n",
            "+-----------------------+------------+--------------------+----------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/Users/tylertrommlitz/miniconda3/envs/cs270/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------+--------------------+----------------+\n",
            "|   # Hidden Layers |   # Epochs |   Training Set MAE |   Test Set MAE |\n",
            "+===================+============+====================+================+\n",
            "|                 1 |        739 |            5.2831  |        5.70672 |\n",
            "+-------------------+------------+--------------------+----------------+\n",
            "|                 2 |        647 |            4.02082 |        5.28987 |\n",
            "+-------------------+------------+--------------------+----------------+\n",
            "|                 3 |        360 |            5.86454 |        5.68052 |\n",
            "+-------------------+------------+--------------------+----------------+\n",
            "|                 4 |        188 |           13.757   |       13.3146  |\n",
            "+-------------------+------------+--------------------+----------------+\n",
            "|                 5 |        260 |           13.7817  |       13.1525  |\n",
            "+-------------------+------------+--------------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "concrete_compressive_strength = fetch_ucirepo(id=165)\n",
        "X = concrete_compressive_strength.data.features\n",
        "y = concrete_compressive_strength.data.targets.to_numpy()\n",
        "\n",
        "# Normalize the data\n",
        "X = normalize(X)\n",
        "\n",
        "table_head = ['Activation Function', '# Epochs', 'Training Set MAE', 'Test Set MAE']\n",
        "table_results = []\n",
        "\n",
        "activation_functions = ['logistic', 'tanh', 'relu']\n",
        "\n",
        "for i in range(3):\n",
        "  clf = MLPRegressor(\n",
        "    hidden_layer_sizes=[64], activation=activation_functions[i], solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=.01, shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000\n",
        "  )\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf.fit(X_train, y_train)\n",
        "  table_results.append([activation_functions[i], clf.n_iter_, mean_absolute_error(clf.predict(X_train), y_train), mean_absolute_error(clf.predict(X_test), y_test)])\n",
        "\n",
        "print(tabulate(table_results, headers=table_head, tablefmt='grid'))\n",
        "\n",
        "# Experiment with more than one hidden layer\n",
        "\n",
        "table_head = ['# Hidden Layers', '# Epochs', 'Training Set MAE', 'Test Set MAE']\n",
        "table_results = []\n",
        "\n",
        "num_hidden_layers = [1, 2, 3, 4, 5]\n",
        "\n",
        "for i in range(5):\n",
        "  clf = MLPRegressor(\n",
        "    hidden_layer_sizes=[64]*num_hidden_layers[i], activation='logistic', solver='sgd',\n",
        "    alpha=0, batch_size=1, learning_rate_init=.01, shuffle=True,\n",
        "    momentum=0, n_iter_no_change=50, max_iter=1000\n",
        "  )\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf.fit(X_train, y_train)\n",
        "  table_results.append([num_hidden_layers[i], clf.n_iter_, mean_absolute_error(clf.predict(X_train), y_train), mean_absolute_error(clf.predict(X_test), y_test)])\n",
        "\n",
        "print(tabulate(table_results, headers=table_head, tablefmt='grid'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HScVQasltlxv"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "The activation functions are used at each hidden node to transform the net input into the output of the node. The logistic function is a sigmoid function that transforms the net input into the range (0, 1). The tanh function is a sigmoid function that transforms the net input into the range (-1, 1). The relu function is a linear function that returns the net input if it is positive, and 0 otherwise. I tested all 3 of these functions to see how the model performed, judging them based on training/test MAE. Logistic and tanh performed similarly, with training MAEs of about 5.28 and 5.54 and test MAEs of 5.18 and 5.71. In comparison, relu performed much worse, with a training MAE of 9.67 and 9.48. I am not sure why the relu function performed so much worse, but it is likely that this dataset is simply not suited for a relu activation function. This highlights the importance of testing different activation functions when tuning a model. In this case, either logistic or tanh would be suitable for this dataset.\n",
        "\n",
        "I also tested using more than one hidden layer. I tested using up to 5 hidden layers, with each layer containing 64 hidden nodes. 1, 2, and 3 hidden layers all performed similarly, with training MAEs of 5.3, 4.0. and 5.9 and test MAEs of 5.7, 5.3, and 5.7 respectively. However, when testing 4 and 5 hidden layers, the MAEs jumped significantly. Their training MAEs were both 13.8 and their test MAEs were 13.3 and 13.2 respectively. \n",
        "\n",
        "A jump from MAEs of around 5 to MAEs of around 13 is significant, especially considering that the only change was adding an extra hidden layer. One hypothesis of mine is that the model is overfitting. However, I do not think this is the case because the training MAE also jumped significantly. If the model was overfitting, we would expect the training MAE to be low and the test MAE to be high. I am not sure why the model performed so much worse with 4 and 5 hidden layers. One thought of mine is that there is simply not enough data for proper training with 4 and 5 hidden layers. There are 1030 instances in the entire dataset, which is lessened when split into training and test sets. This is not a lot of data to train a model, and it is possible that training is breaking down when too many hidden layers are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 4. (Optional 20% extra credit) Code up your own MLP/Backprop learner \n",
        "Below is a scaffold you could use if you want. Requirements for this task:\n",
        "- Your model should support the methods shown in the example scaffold below.\n",
        "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes. You may choose just one activation function for all hidden and output nodes if you want (e.g. logistic activation function where the loss is SSE rather than cross-entropy).\n",
        "- Random weight initialization with small random weights with 0 mean. Remember that every hidden and output node should have its own bias weight.\n",
        "- Use stochastic training updates: update weights after each training instance (i.e. not batch)\n",
        "- Option to include a momentum term\n",
        "- Your class can inherit from the relevant scikit-learn learners (e.g. data shuffling, etc.), but don't call any of the super methods to accomplish the core methods in the scaffold.\n",
        "- Run the Iris data set above with your Backprop version. Show and discuss your results and how they compare with the sklearn version.\n",
        "- Coding MLP is a good experience but is a little more challening that implementing other models so the extra credit points are higher than typical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rokMDC3Qtlxv"
      },
      "source": [
        "*Discuss your results and any differences*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hn8n_iR8tlxv"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class MLP(BaseEstimator,ClassifierMixin):\n",
        "    def __init__(self,lr=.1, momentum=0, shuffle=True,hidden_layer_widths=None):\n",
        "        \"\"\" Initialize class with chosen hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            lr (float): A learning rate / step size.\n",
        "            shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
        "            momentum(float): The momentum coefficent \n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            hidden_layer_widths (list(int)): A list of integers which defines the width of each hidden layer if hidden layer is none do twice as many hidden nodes as input nodes. (and then one more for the bias node)\n",
        "            For example: input width 1, then hidden layer will be 3 nodes\n",
        "        Example:\n",
        "            mlp = MLP(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
        "        \"\"\"\n",
        "        self.hidden_layer_widths\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "\n",
        "    def fit(self, X, y, initial_weights=None):\n",
        "        \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 2D numpy array with the training targets\n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            initial_weights (array-like): allows the user to provide initial weights\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "        \"\"\"\n",
        "        self.weights = self.initialize_weights() if not initial_weights else initial_weights\n",
        "        \n",
        "        # your code here\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return [0]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 2D numpy array with targets\n",
        "\n",
        "        Returns:\n",
        "            score : float\n",
        "                Mean accuracy of self.predict(X) wrt. y.\n",
        "        \"\"\"\n",
        "\n",
        "        return 0\n",
        "\n",
        "    ###  Returns the weights. Not required but helpful for debugging\n",
        "    def get_weights(self):\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2.7.16 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
